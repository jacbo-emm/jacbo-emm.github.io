<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Flink窗口联结</title>
      <link href="/post/31b6945a.html"/>
      <url>/post/31b6945a.html</url>
      
        <content type="html"><![CDATA[<h3 id="Flink窗口联结"><a href="#Flink窗口联结" class="headerlink" title="Flink窗口联结"></a>Flink窗口联结</h3><h4 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h4><p>​对于两条流的合并，很多情况我们并不是简单地将所有数据放在一起，而是希望根据某个字段的值将它们联结起来，“配对”去做处理。</p><p>​join都是利用window的机制，即按照指定字段和（滚动&#x2F;滑动&#x2F;会话）窗口进行inner join，先将数据缓存在Window State中，当窗口触发计算时，执行join操作；按照窗口的操作和类型可以分为：Tumbling Window Join、Sliding Window Join、Session Widnow Join。</p><p>​两条流的数据到来之后，首先会按照 key 分组、进入对应的窗口中存储；当到达窗口结束时间时，算子会先统计出窗口内两条流的数据的所有组合，也就是对两条流中的数据做一个笛卡尔积（相当于表的交叉连接，cross join），然后进行遍历，把每一对匹配的数据，作为参数(first，second)传入 JoinFunction 的.join()方法进行计算处理，得到的结果直接输出如下图所示。所以窗口中每有一对数据成功联匹配，JoinFunction 的.join()方法就会被调用一次，并输出一个结果。</p><p>其实仔细观察可以发现，窗口 join 的调用语法和我们熟悉的 SQL 中表的 join 非常相似,将每条流当作一张表，键选择器当作join的条件：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> table1 t1, table2 t2 <span class="keyword">WHERE</span> t1.id <span class="operator">=</span> t2.id;</span><br></pre></td></tr></table></figure><h4 id="CoGroup"><a href="#CoGroup" class="headerlink" title="CoGroup"></a>CoGroup</h4><p>​window co-group operation 是一种将两个流中的元素按 key 和 window 分组的操作。它类似于窗口连接，但不是连接元素，而是将它们组合在一起并将用户定义的函数应用于元素组。它的用法跟 window join 非常类似，也是将两条流合并之后开窗处理匹配的元素，调用时只需要将.join()换为.coGroup()。</p><p>​coGroup 操作比窗口的 join 更加通用，不仅可以实现类似 SQL 中的“内连接”（inner join），也可以实现左外连接（left outer join）、右外连接（right outer join）和全外连接（full outer join）。事实上，窗口 join 的底层，也是通过 coGroup 来实现的。</p><h4 id="Interval-Join"><a href="#Interval-Join" class="headerlink" title="Interval Join"></a>Interval Join</h4><p>​ Interval Join是一种基于元素的时间戳应用于两个流的连接操作。间隔连接根据用户定义的间隔组合来自两个流的元素，并将用户定义的函数应用于每对匹配元素。间隔连接类似于窗口连接，但它不是使用预定义的窗口，而是使用用户定义的间隔来匹配来自两个输入流的元素。</p><p>​    在有些场景下，我们要处理的时间间隔可能并不是固定的。比如，在交易系统中，需要实时地对每一笔交易进行核验，保证两个账户转入转出数额相等，也就是所谓的“实时对账”。两次转账的数据可能写入了不同的日志流，它们的时间戳应该相差不大，所以我们可以考虑只统计一段时间内是否有出账入账的数据匹配。这时显然不应该用滚动窗口或滑动窗口来处理— —因为匹配的两个数据有可能刚好“卡在”窗口边缘两侧，于是窗口内就都没有匹配了；会话窗口虽然时间不固定，但也明显不适合这个场景。 基于时间的窗口联结已经无能为力了。</p><p>​    为了应对这样的需求，Flink 提供了一种叫作“间隔联结”（interval join）的合流操作。顾名思义，间隔联结的思路就是针对一条流的每个数据，开辟出其时间戳前后的一段时间间隔，看这期间是否有来自另一条流的数据匹配。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Backpressure</title>
      <link href="/post/9d92a29a.html"/>
      <url>/post/9d92a29a.html</url>
      
        <content type="html"><![CDATA[<h3 id="Flink-Backpressure"><a href="#Flink-Backpressure" class="headerlink" title="Flink Backpressure"></a>Flink Backpressure</h3><h4 id="Flink反压原因"><a href="#Flink反压原因" class="headerlink" title="Flink反压原因"></a>Flink反压原因</h4><p>​当数据源的生产速度过快，超过 Flink 处理速度时，Flink 会尝试缓存数据以等待处理，但如果数据源持续生产数据，缓存区将会满，这时 Flink 就需要采取反压机制来限制数据源的生产速度，直到 Flink 处理完所有已缓存的数据并释放出足够的缓存空间。这样可以避免系统崩溃或丢失数据的情况出现。反压机制通常使用一些调节流量的算法或策略，例如基于水位线的反压机制，它根据当前缓存区中的数据量动态调整数据源的生产速度，以平衡数据源和 Flink 处理速度之间的差异。</p><p><strong>系统资源</strong></p><p>​首先，需要检查机器的资源使用情况，像CPU、网络、磁盘I&#x2F;O等。如果一些资源负载过高，就可以进行下面的处理：</p><ul><li>尝试优化代码；</li><li>针对特定资源对Flink进行调优；</li><li>增加并发或者增加机器</li></ul><p><strong>垃圾回收</strong></p><p>​性能问题常常源自过长的GC时长。这种情况下可以通过打印GC日志，或者使用一些内存&#x2F;GC分析工具来定位问题。<strong>CPU&#x2F;线程瓶颈</strong></p><p>​有时候，如果一个或者一些线程造成CPU瓶颈，而此时，整个机器的CPU使用率还相对较低，这种CPU瓶颈不容易发现。比如，如果一个48核的CPU，有一个线程成为瓶颈，这时CPU的使用率只有2%。这种情况下可以考虑使用代码分析工具来定位热点线程。</p><p><strong>线程争用</strong></p><p>​跟上面CPU&#x2F;线程瓶颈问题类似，一个子任务可能由于对共享资源的高线程争用成为瓶颈。同样的，CPU分析工具对于探查这类问题也很有用。</p><p><strong>负载不均</strong></p><p>​如果瓶颈是数据倾斜造成的，可以尝试删除倾斜数据，或者通过改变数据分区策略将造成数据的key值拆分，或者也可以进行本地聚合&#x2F;预聚合。上面几项并不是全部场景。通常，解决数据处理过程中的瓶颈问题，进而消除反压，首先需要定位问题节点（瓶颈所在），然后找到原因，寻找原因，一般从检查资源过载开始。</p><h4 id="Flink网络流控和反压机制"><a href="#Flink网络流控和反压机制" class="headerlink" title="Flink网络流控和反压机制"></a>Flink网络流控和反压机制</h4><p>​网络流控是为了在上下游速度不匹配的情况下，如何防止下游出现过载</p><p>​网络流控有静态限速和动态反压两种手段</p><p>​Flink1.5以前是基于TCP流控 + bounded buffer 来实现反压</p><p>​Flink1.5之后实现了自己托管的credit-based流控机制，在应用层模拟TCP流控的机制</p>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>窗口和水位线案例</title>
      <link href="/post/f290c767.html"/>
      <url>/post/f290c767.html</url>
      
        <content type="html"><![CDATA[<h4 id="窗口和水位线案例"><a href="#窗口和水位线案例" class="headerlink" title="窗口和水位线案例"></a>窗口和水位线案例</h4><p>​水位线 watermark，指的是 Flink底层在数据流中添加的带有时间戳的数据，在 Flink 流处理中，Watermark 是用于标记事件时间进展的特殊事件，它会告诉 Flink 系统事件时间已经到达了哪个阶段，并将此信息应用于处理窗口和迟到的事件。在时间窗口处理中，只有当 Watermark 时间戳大于或等于窗口结束时间时，才会触发窗口计算；（数据可以理解为 一条日志，或传感器采集的信息）</p><p>​作用： 水位线可以用来处理无序数据流（下文代码例子会给出）</p><p>​如何产生水位线？</p><p>​指定水位线的时间戳如何获取？ 可以指定 水位线时间戳从业务数据（抽象为 JavaBean）的某个属性获取；<br>​指定水位线可以延迟多长时间，即允许无序数据最多可以晚来多长时间；（超过这个时间会被丢弃）</p><p><strong>传感器实体类：</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.math.BigDecimal;</span><br><span class="line"><span class="keyword">import</span> java.sql.Timestamp;</span><br><span class="line"><span class="keyword">import</span> java.text.ParseException;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Calendar;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SensorReadingTimeWatermarkWindow</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> String id;</span><br><span class="line">    <span class="keyword">private</span> String type;</span><br><span class="line">    <span class="keyword">private</span> Timestamp timestamp;</span><br><span class="line">    <span class="keyword">private</span> BigDecimal temperature;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">SensorReadingTimeWatermarkWindow</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">SensorReadingTimeWatermarkWindow</span><span class="params">(String id, String type, String timeStr, BigDecimal temperature)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = id;</span><br><span class="line">        <span class="built_in">this</span>.type = type;</span><br><span class="line">        <span class="built_in">this</span>.temperature = temperature;</span><br><span class="line">        <span class="built_in">this</span>.parseTimestamp(timeStr);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">parseTimestamp</span><span class="params">(String timeStr)</span>  &#123;</span><br><span class="line">        <span class="type">SimpleDateFormat</span> <span class="variable">simpleDateFormat</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="built_in">this</span>.timestamp = <span class="keyword">new</span> <span class="title class_">Timestamp</span>(simpleDateFormat.parse(timeStr).getTime());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ParseException e) &#123;</span><br><span class="line">            <span class="built_in">this</span>.timestamp = <span class="keyword">new</span> <span class="title class_">Timestamp</span>(System.currentTimeMillis());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 省略了 getter 和 setter 方法</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.WatermarkStrategy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.AggregateFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.example.pojo.SensorReadingTimeWatermarkWindow;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.math.BigDecimal;</span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">EventTimeWatermarkWindow</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建执行环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">environment</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        environment.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从socket中读取数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; textStream = environment.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;SensorReadingTimeWatermarkWindow&gt; sensorStream = textStream.map(x -&gt; &#123;</span><br><span class="line">            String[] arr = x.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SensorReadingTimeWatermarkWindow</span>(arr[<span class="number">0</span>], arr[<span class="number">1</span>], arr[<span class="number">2</span>], <span class="keyword">new</span> <span class="title class_">BigDecimal</span>(arr[<span class="number">3</span>]));</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置抽取时间戳，水位线延迟2秒（如当前时间戳为 20:00:10 ，水位线的时间是 20:00:08），窗口是看水位线时间，而不是时间时间</span></span><br><span class="line">        SingleOutputStreamOperator&lt;SensorReadingTimeWatermarkWindow&gt; streamWithWatermark = sensorStream.assignTimestampsAndWatermarks(</span><br><span class="line">                WatermarkStrategy.&lt;SensorReadingTimeWatermarkWindow&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">2</span>))</span><br><span class="line">                        .withTimestampAssigner((event, timestamp) -&gt; event.getTimestamp().getTime())</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开窗聚合</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; aggForWindowStream = streamWithWatermark.keyBy(SensorReadingTimeWatermarkWindow::getType)</span><br><span class="line">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .aggregate(<span class="keyword">new</span> <span class="title class_">AggregateFunction</span>&lt;SensorReadingTimeWatermarkWindow, String, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> String <span class="title function_">createAccumulator</span><span class="params">()</span> &#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> String <span class="title function_">add</span><span class="params">(SensorReadingTimeWatermarkWindow sensorReadingTimeWatermarkWindow, String acc)</span> &#123;</span><br><span class="line">                        <span class="keyword">return</span> acc + <span class="string">&quot; &quot;</span> + sensorReadingTimeWatermarkWindow.getId();</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> String <span class="title function_">getResult</span><span class="params">(String acc)</span> &#123;</span><br><span class="line">                        <span class="keyword">return</span> acc;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> String <span class="title function_">merge</span><span class="params">(String s, String acc)</span> &#123;</span><br><span class="line">                        <span class="keyword">return</span> s + <span class="string">&quot; &quot;</span> + acc;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        aggForWindowStream.print(<span class="string">&quot;aggForWindowStream&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        environment.execute();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1,sensor1,2022-04-17 22:07:01,36.1</span><br><span class="line"> </span><br><span class="line">7,sensor1,2022-04-17 22:07:02,36.7</span><br><span class="line"> </span><br><span class="line">8,sensor1,2022-04-17 22:07:04,36.8</span><br><span class="line"> </span><br><span class="line">11,sensor1,2022-04-17 22:07:07,36.9</span><br><span class="line"> </span><br><span class="line">12,sensor1,2022-04-17 22:07:11,36.9  -&gt; 1, 7, 8, 11 </span><br><span class="line"> </span><br><span class="line">13,sensor1,2022-04-17 22:07:09,36.9</span><br><span class="line"> </span><br><span class="line">15,sensor1,2022-04-17 22:07:16,36.9 </span><br><span class="line"> </span><br><span class="line">16,sensor1,2022-04-17 22:07:23,36.9 -&gt; 12, 15 </span><br></pre></td></tr></table></figure><h5 id="事件迟到被丢弃"><a href="#事件迟到被丢弃" class="headerlink" title="事件迟到被丢弃"></a>事件迟到被丢弃</h5><p>【结果分析】</p><p>​发现1：当事件12（id&#x3D;12）出现时，因水位线延迟时间为0，所以水位线时间戳等于事件12的时间戳&#x3D;22:07:11，这个时间戳大于窗口结束时间（22:07:10），第1个窗口被关闭(销毁)并输出计算结果为【1，7，8，11】；</p><p>​发现2： 当事件16（id&#x3D;16）出现时，因水位线延迟时间为0，所以水位线时间戳等于事件16的时间戳&#x3D;22:07:23，这个时间戳大于窗口结束时间（22:07:20），第2个窗口被关闭并输出计算结果为【12，15】；</p><p>​发现3：事件13没有更新水位线，因为水位线必须单调递增（事件12发生时的水位线是22:07:11，事件13的时间戳是22:07:09，所以事件13发生时不会更新水位线）；</p><p>​问题来了： 事件13去哪里了？ 被 Flink 丢弃了，因为事件13迟到了；</p><p>​如何理解事件迟到了： 因为事件12 的时间戳为 22:07:11，又水位线延迟0s，所以水位线的 时间戳也是 22:07:11，这大于窗口结束时间，所以窗口关闭并计算结果，窗口关闭后，事件13才来，因此被丢弃。</p><p>【补充】窗口范围是左闭右开；如上图，第1个窗口的范围是 [0,10)，第2个窗口是 [10,20)</p><h5 id="事件迟到但被正常处理"><a href="#事件迟到但被正常处理" class="headerlink" title="事件迟到但被正常处理"></a>事件迟到但被正常处理</h5><p>​修改上述水位线代码， 设置延迟时间为5s，重新录入数据，结果如下：</p><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">1,sensor1,2022-04-17 22:07:01,36.1</span><br><span class="line"> </span><br><span class="line">7,sensor1,2022-04-17 22:07:02,36.7</span><br><span class="line"> </span><br><span class="line">8,sensor1,2022-04-17 22:07:04,36.8</span><br><span class="line"> </span><br><span class="line">11,sensor1,2022-04-17 22:07:07,36.9</span><br><span class="line"> </span><br><span class="line">12,sensor1,2022-04-17 22:07:11,36.9  </span><br><span class="line"> </span><br><span class="line">13,sensor1,2022-04-17 22:07:09,36.9</span><br><span class="line"> </span><br><span class="line">15,sensor1,2022-04-17 22:07:16,36.9 -&gt; 1, 7, 8, 11, 13 </span><br><span class="line"> </span><br><span class="line">16,sensor1,2022-04-17 22:07:23,36.9 </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">21,sensor1,2022-04-17 22:07:20,36.9</span><br><span class="line"> </span><br><span class="line">22,sensor1,2022-04-17 22:07:25,36.9 -&gt; 12, 15</span><br></pre></td></tr></table></figure><p>【结果分析】</p><p>​发现1：事件13，事件21 不会更新水位线时间戳，原因上文已经解释过了；</p><p>​发现2：当事件15（id&#x3D;15）出现时，因水位线延迟时间为5s，所以水位线等于事件15的时间戳减去5s &#x3D; 22:07:11，这个时间戳大于窗口结束时间（22:07:10），第1个窗口被关闭并输出计算结果为【1，7，8，11，13】；</p><p>​发现3：事件13没有被丢弃，因为水位线延迟了5s，窗口在事件15发生时才关闭，所以可以探测到事件13，这也阐述了为啥 Flink水位线可以处理无序数据的原理；</p><p>​发现4：当事件22（id&#x3D;22）出现时，因水位线延迟时间为5s，所以水位线等于事件22的时间戳减去5s &#x3D; 22:07:20，这个时间戳大于等于窗口结束时间（22:07:20），第2个窗口被关闭并输出计算结果为【12，15】；（<strong>大于等于窗口结束时间，窗口就被关闭，因为窗口范围是左闭右开</strong>）</p><h5 id="窗口的-lateness-延迟属性"><a href="#窗口的-lateness-延迟属性" class="headerlink" title="窗口的 lateness 延迟属性"></a>窗口的 lateness 延迟属性</h5><p>​此外，窗口还有 <em><strong>lateness</strong></em> 属性，表示延迟多长时间关闭窗口；如下面代码每10s 创建一个长度为12s的窗口； （如果没有 lateness参数或其为0的话， 就是 每10s 创建一个长度为10s的窗口）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1,sensor1,2022-04-17 22:07:01,36.1</span><br><span class="line"> </span><br><span class="line">7,sensor1,2022-04-17 22:07:02,36.7</span><br><span class="line"> </span><br><span class="line">8,sensor1,2022-04-17 22:07:04,36.8</span><br><span class="line"> </span><br><span class="line">11,sensor1,2022-04-17 22:07:07,36.9</span><br><span class="line"> </span><br><span class="line">12,sensor1,2022-04-17 22:07:11,36.9</span><br><span class="line"> </span><br><span class="line">13,sensor1,2022-04-17 22:07:09,36.9</span><br><span class="line"> </span><br><span class="line">15,sensor1,2022-04-17 22:07:15,36.9 -&gt; 1, 7, 8, 11, 13</span><br><span class="line"> </span><br><span class="line">16,sensor1,2022-04-17 22:07:09,36.9 -&gt; 1, 7, 8, 11, 13, 16 </span><br><span class="line"> </span><br><span class="line">17,sensor1,2022-04-17 22:07:16,36.9 </span><br><span class="line"> </span><br><span class="line">18,sensor1,2022-04-17 22:07:09,36.9 -&gt; 1, 7, 8, 11, 13, 16, 18 </span><br><span class="line"> </span><br><span class="line">19,sensor1,2022-04-17 22:07:17,36.9 窗口关闭 </span><br><span class="line"> </span><br><span class="line">20,sensor1,2022-04-17 22:07:09,36.9  被丢弃 </span><br><span class="line"> </span><br><span class="line">21,sensor1,2022-04-17 22:07:20,36.9</span><br><span class="line"> </span><br><span class="line">22,sensor1,2022-04-17 22:07:25,36.9 -&gt; 12, 15, 17, 19 </span><br></pre></td></tr></table></figure><p>【结果分析】</p><p>​事件15发生时：因水位线延迟5s，所以水位线时间戳&#x3D;22:07:15-5s&#x3D;22:07:10，等于第1个窗口的结束时间，故第1个窗口计算，结果为 【1, 7, 8, 11, 13】，但窗口没有关闭，因为lateness为2s，延迟2秒关闭，即当水位线大于等于 22:07:12 时，窗口关闭；</p><p>​事件16发生时：第1个窗口因为 lateness&#x3D;2s 没有关闭，又事件16时间戳&#x3D;22:07:09，所以还是参与窗口1的计算，输出结果【1, 7, 8, 11, 13, 16】；</p><p>​事件17发生时：时间戳&#x3D;22:07:16，水位线时间戳&#x3D;22:07:11，这小于带lateness&#x3D;2s的窗口1的关闭时间 22:07:12，所以窗口1还是不会关闭；</p><p>​事件18发生时：时间戳&#x3D;22:07:09， 因水位线单调递增，故不变，还是22:07:11；事件18参与窗口1的计算，结果为 【1, 7, 8, 11, 13, 16, 18】</p><p>​事件19发生时：时间戳&#x3D;22:07:17，水位线&#x3D;22:07:12，等于带lateness&#x3D;2s的窗口1的关闭时间，窗口1关闭；</p><p>​事件20发生时：时间戳&#x3D;22:07:09，落入了窗口1的范围（22:07:00~22:07:10），但因窗口1已经关闭，所以事件20被丢弃；</p><p><strong>【问题】</strong> 事件20被丢弃的话， 不满足业务场景对数据一致性的要求；</p><p>​因为服务1发送了10条数据，到达服务2的时候却只有9条数据，这不满足业务需求，是开发团队不愿意看到的事情；<em><strong>那如何找回这些被丢弃的事件呢</strong></em>？<strong>通过侧输出</strong>；</p><h5 id="如何收集迟到数据"><a href="#如何收集迟到数据" class="headerlink" title="如何收集迟到数据"></a>如何收集迟到数据</h5><p>​从旁路输出（side output）获取迟到数据；通过 Flink 的 旁路输出 功能，可以获得迟到数据的数据流。首先，需要在开窗后的 stream 上使用 sideOutputLateData(OutputTag) 表明需要把迟到数据存入 旁输出流。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1,sensor1,2022-04-17 22:07:01,36.1</span><br><span class="line"> </span><br><span class="line">7,sensor1,2022-04-17 22:07:02,36.7</span><br><span class="line"> </span><br><span class="line">8,sensor1,2022-04-17 22:07:04,36.8</span><br><span class="line"> </span><br><span class="line">11,sensor1,2022-04-17 22:07:07,36.9</span><br><span class="line"> </span><br><span class="line">12,sensor1,2022-04-17 22:07:11,36.9  </span><br><span class="line"> </span><br><span class="line">13,sensor1,2022-04-17 22:07:09,36.9</span><br><span class="line"> </span><br><span class="line">15,sensor1,2022-04-17 22:07:15,36.9 -&gt; 1, 7, 8, 11, 13</span><br><span class="line"> </span><br><span class="line">16,sensor1,2022-04-17 22:07:09,36.9 -&gt; 1, 7, 8, 11, 13, 16 </span><br><span class="line"> </span><br><span class="line">17,sensor1,2022-04-17 22:07:16,36.9 </span><br><span class="line"> </span><br><span class="line">18,sensor1,2022-04-17 22:07:09,36.9 -&gt; 1, 7, 8, 11, 13, 16, 18 </span><br><span class="line"> </span><br><span class="line">19,sensor1,2022-04-17 22:07:17,36.9 窗口关闭 </span><br><span class="line"> </span><br><span class="line">20,sensor1,2022-04-17 22:07:09,36.9  -&gt; lateOutputTag&gt; SensorReadingTimeWindow&#123;id=&#x27;20&#x27;, type=&#x27;sensor1&#x27;, timestamp=2022-04-17 22:07:09.0, temperature=36.9&#125; </span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink状态</title>
      <link href="/post/e6ec0e77.html"/>
      <url>/post/e6ec0e77.html</url>
      
        <content type="html"><![CDATA[<h3 id="Flink状态"><a href="#Flink状态" class="headerlink" title="Flink状态"></a>Flink状态</h3><h4 id="状态定义"><a href="#状态定义" class="headerlink" title="状态定义"></a>状态定义</h4><p>​<strong>无状态算子：</strong></p><p>​无状态的算子任务只需要观察每个独立事件，根据当前输入的数据直接转换输出结果。如map、filter、flatMap，计算时不依赖其他数据，就都属于无状态的算子。</p><p>​<strong>有状态算子：</strong></p><p>​当前数据之外，还需要一些其他数据来得到计算结果。这里的“其他数据”，就是所谓的状态（state）比如，做求和（sum）计算时，需要保存之前所有数据的和，这就是状态；</p><p>​实时任务失败：重启任务，然后重新读一遍输入数据，最后把昨天数据重新计算一遍就不可以了？因为实时任务第一重要的就是时效性，很明显重新计算违背了时效性原则。</p><h4 id="托管方式"><a href="#托管方式" class="headerlink" title="托管方式"></a>托管方式</h4><table><thead><tr><th></th><th>Managed State</th><th>Raw State</th></tr></thead><tbody><tr><td>状态管理方式</td><td>Flink Runtime托管，自动存储、自动恢复，内存管理上有优化</td><td>用户自己管理,需要自己进行序列化</td></tr><tr><td>数据结构</td><td>Flink提供的常用数据结构,如ListState、MapState、ValueState等</td><td>字节数组: byte[]</td></tr><tr><td>使用场景</td><td>大多数情况下均可使用</td><td>自定义 Operator 时</td></tr></tbody></table><p>Raw State ：只有在遇到托管状态无法实现的特殊需求时，我们才会考虑使用原始状态；</p><h4 id="状态类型"><a href="#状态类型" class="headerlink" title="状态类型"></a>状态类型</h4><p>​在Flink中，一个算子任务会按照并行度分为多个并行子任务执行，而不同的子任务会占据不同的任务槽（task slot）。由于不同的slot在计算资源上是物理隔离的，所以Flink能管理的状态在并行任务间是无法共享的，每个状态只能针对当前子任务的实例有效。</p><p>​而很多有状态的操作（比如聚合、窗口）都是要先做keyBy进行按键分区的。按键分区之后，任务所进行的所有计算都应该只针对当前key有效，所以状态也应该按照key彼此隔离。在这种情况下，状态的访问方式又会有所不同。基于这样的想法，我们又可以将托管状态分为两类：算子状态和按键分区状态。</p><table><thead><tr><th></th><th>Keyed State</th><th>Operator State</th></tr></thead><tbody><tr><td>使用范围</td><td>只能用于KeyedStream算子中使用，每个Key对应一个state</td><td>可以用于所有算子，常用与Source，比如FlinkKafkaConsumer，一个Operator实例对应一个state</td></tr><tr><td>扩缩容模式</td><td>Flink把所有键值分为不同的 Key Group，的最小单元。并发改变时，Flink会以KeyGroup为单位将键值分配给不同的任务。</td><td>当并发改变时，有多种方式来进行重分BroadcastState会把状态拷贝到全部新任务上。</td></tr><tr><td>访问方式</td><td>实现Rich Function，通过getRuntimeContext()返回的RuntimeContext进行获取</td><td>实现CheckpointedFunction或者ListCheckpoint的接口</td></tr><tr><td>数据结构</td><td>ValueState、ListState、 ReducingState.AggregatingState 、MapState</td><td>ListState、BroadcastState</td></tr></tbody></table><p>算子状态(Operator State)：</p><p>​算子状态可以用在所有算子上，使用的时候其实就跟一个本地变量没什么区别，因为本地变量的作用域也是当前任务实例。在使用时，我们还需进一步实现CheckpointedFunction接口。</p><p> <strong>键分区状态(Keyed State)：</strong></p><p>状态是根据输入流中定义的键（key）来维护和访问的，所以只能定义在按键分区流（KeyedStream）中，也就是聚合算子必须在keyBy之后才能使用，因为聚合的结果是以Keyed State的形式保存的。</p><h4 id="状态后端"><a href="#状态后端" class="headerlink" title="状态后端"></a>状态后端</h4><p>一个 State Backend 主要负责两件事：Local State Management(本地状态管理) 和 Remote State Checkpointing（远程状态备份）。</p><p>​<strong>Local State Management：</strong></p><p>​1、Local State Management 主要任务是确保状态的更新和访问。</p><p>​2、Local State Management 主要有两种形式的状态管理：</p><p>​① 直接将 State 以对象的形式存储到JVM的堆上面，因为是在内存中读写，延迟会很低，但State的大小受限于内存的大小</p><p>​② 将 State 对象序列化后存储到 RocksDB 中，读写较内存会慢一些，但不受内存大小的限制，同时因为state存储在磁盘上，可以减少应用程序对内存的占用</p><p>​<strong>Remote State Checkpointing：</strong></p><p>​Flink程序是分布式运行的，而State都是存储到各个节点上的，一旦TaskManager节点出现问题，就会导致State的丢失。State Backend 提供了 State Checkpointing 的功能，将 TaskManager 本地的 State 的备份到远程的存储介质上，可以是分布式的存储系统或者数据库。不同的 State Backends 备份的方式不同，会有效率高低的区别。</p><p>​Remote State Checkpointing 的主要作用包括在每一个TaskManager节点上存储和管理状态，将状态进行远程备份两个部分。</p><p><strong>Flink-1.13 版及以前存储方式：</strong></p><p><strong>Flink-1.13 版及以后存储方式：</strong></p><p>​<strong>HashMapStateBackend</strong>（默认），FsStateBackend 和 MemoryStateBackend 整合成了 HashMapStateBackend存储管理状态类似于管理一个Java堆中的对象，key&#x2F;value 的状态和窗口操作都会在一个 hash table 中进行存储状态。默认情况下，每一个状态最大为 5 MB。可以通过 MemoryStateBackend 的构造函数增加最大大小。内存空间不够时，也会溢出一部分数据到本地磁盘文件；可以支撑大规模的状态数据，只不过在状态数据规模超出内存空间时，读写效率就会明显降低。</p><p>​<strong>EmbeddedRocksDBStateBackend</strong>，该状态后端存储管理状态在RocksDB数据库中，该状态存储在本地 TaskManager的磁盘目录。该状态后端按照指定的类型序列化后变成字节数组将数据存储在磁盘，按照key的字典序排列。RocksDB 的每个 key 和 value 的最大大小为 2^31 字节。这是因为 RocksDB 的 JNI API 是基于 byte[] 的。Rockdb 中的数据，有内存缓存的部分，也有磁盘文件的部分；Rockdb 的磁盘文件数据读写速度相对还是比较快的，所以在支持超大规模状态数据时，数据的读写效率不会有太大的降低。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink算子</title>
      <link href="/post/82ff8bac.html"/>
      <url>/post/82ff8bac.html</url>
      
        <content type="html"><![CDATA[<h3 id="Flink-Source类算子"><a href="#Flink-Source类算子" class="headerlink" title="Flink Source类算子"></a>Flink Source类算子</h3><h4 id="基于文件"><a href="#基于文件" class="headerlink" title="基于文件"></a>基于文件</h4><p>​readTextFile(path) ：读取文本文件，例如遵守 TextInputFormat 规范的文件，逐行读取并将它们作为字符串返回。</p><p>​readFile(fileInputFormat, path) ： 按照指定的文件输入格式读取（一次）文件。</p><h4 id="基于Socket"><a href="#基于Socket" class="headerlink" title="基于Socket"></a>基于Socket</h4><p>​socketTextStream ：从套接字读取。元素可以由分隔符分隔。在启动 Flink 程序之前， 必须先启动一个 Socket 服务</p><h4 id="基于集合"><a href="#基于集合" class="headerlink" title="基于集合"></a>基于集合</h4><p>​fromCollection(Collection) ：从 Java Java.util.Collection 创建数据流。集合中的所有元素必须属于同一类型。</p><p>​fromCollection(Iterator, Class) ：从迭代器创建数据流。class 参数指定迭代器返回元素的数据类型。</p><p>​fromElements(T …) ： 从给定的对象序列中创建数据流。所有的对象必须属于同一类型。</p><p>​fromParallelCollection(SplittableIterator, Class) ：从迭代器并行创建数据流。class 参数指定迭代器返回元素的数据类型。</p><p>​generateSequence(from, to) ：基于给定间隔内的数字序列并行生成数据流。</p><h4 id="基于Connectors"><a href="#基于Connectors" class="headerlink" title="基于Connectors"></a>基于Connectors</h4><p>​预定义 data sources 支持从文件、目录、socket，以及 collections 和 iterators 中读取数据。</p><h3 id="Flink-Transformation类算子"><a href="#Flink-Transformation类算子" class="headerlink" title="Flink Transformation类算子"></a>Flink Transformation类算子</h3><h4 id="map和flatMap"><a href="#map和flatMap" class="headerlink" title="map和flatMap"></a>map和flatMap</h4><p>​flatMap 算子会将每条输入记录映射为多条输出记录，并将这些输出记录展开成一个扁平的数据集。与之相对的算子是 map 算子，它只将每条输入记录映射为一条输出记录。</p><h4 id="keyBy"><a href="#keyBy" class="headerlink" title="keyBy"></a>keyBy</h4><p>​在逻辑上将流划分为不相交的分区。具有相同 key 的记录都分配到同一个分区。在内部， <em>keyBy()</em> 是通过哈希分区实现的。有多种指定 key 的方式。</p><p>​以下情况，一个类<strong>不能作为 key</strong>：</p><ol><li>它是一种 POJO 类，但没有重写 hashCode() 方法而是依赖于 Object.hashCode() 实现。</li><li>它是任意类的数组。</li></ol><h4 id="aggregation"><a href="#aggregation" class="headerlink" title="aggregation"></a>aggregation</h4><p>​滚动聚合算子由 KeyedStream 调用，并生成一个聚合以后的DataStream滚动聚合算子是多个聚合算子的统称， 有 sum、 min、 minBy、 max、 maxBy；</p><p>​滚动聚合方法：</p><p>​sum()：在输入流上对指定的字段做滚动相加操作。</p><p>​min()：在输入流上对指定的字段求最小值。</p><p>​max()：在输入流上对指定的字段求最大值。</p><p>​minBy()：在输入流上针对指定字段求最小值，并返回最小值字段所在的那条数据。</p><p>​maxBy()：在输入流上针对指定字段求最大值，并返回最大值字段所在的那条数据。</p><h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h4><p>​在相同 key 的数据流上“滚动”执行 reduce。将当前元素与最后一次 reduce 得到的值组合然后输出新值。</p><h4 id="iterate"><a href="#iterate" class="headerlink" title="iterate"></a>iterate</h4><p>​通过将一个算子的输出重定向到某个之前的算子来在流中创建“反馈”循环。这对于定义持续更新模型的算法特别有用。下面的代码从一个流开始，并不断地应用迭代自身。大于 0 的元素被发送回反馈通道，其余元素被转发到下游。</p><h4 id="union和connect"><a href="#union和connect" class="headerlink" title="union和connect"></a>union和connect</h4><p>​在<code>DataStream</code>上使用<code>union</code>算子可以合并多个同类型的数据流，并生成同类型的数据流，即可以将多个<code>DataStream[T]</code>合并为一个新的<code>DataStream[T]</code>。数据将按照先进先出（First In First Out）的模式合并，且不去重。下图<code>union</code>对白色和深色两个数据流进行合并，生成一个数据流。</p><p>​<code>union</code>虽然可以合并多个数据流，但有一个限制，即多个数据流的数据类型必须相同。<code>connect</code>提供了和<code>union</code>类似的功能，用来连接两个数据流，它与<code>union</code>的区别在于：</p><ol><li><code>connect</code>只能连接两个数据流，<code>union</code>可以连接多个数据流。</li><li><code>connect</code>所连接的两个数据流的数据类型可以不一致，<code>union</code>所连接的两个数据流的数据类型必须一致。</li><li>两个<code>DataStream</code>经过<code>connect</code>之后被转化为<code>ConnectedStreams</code>，<code>ConnectedStreams</code>会对两个流的数据应用不同的处理方法，且双流之间可以共享状态。</li></ol><p><code>connect</code>经常被应用在对一个数据流使用另外一个流进行控制处理的场景上，如下图所示。控制流可以是阈值、规则、机器学习模型或其他参数。</p><h3 id="Flink-Sink类算子"><a href="#Flink-Sink类算子" class="headerlink" title="Flink Sink类算子"></a>Flink Sink类算子</h3><p>​writeToSocket：将计算结果输出到某台机器的端口上。</p><p>​预定义 data sinks 支持把数据写入文件、标准输出（stdout）、标准错误输出（stderr）和 socket</p><h3 id="Flink-Partitioner类算子"><a href="#Flink-Partitioner类算子" class="headerlink" title="Flink Partitioner类算子"></a>Flink Partitioner类算子</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">upperStream.print().setParallelism(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// GlobalPartitioner 分区器会将上游所有元素都发送到下游的第一个算子实例上(SubTask id = 0)</span></span><br><span class="line">upperStream.global().print(<span class="string">&quot;GlobalPartitioner&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据会被循环发送到下游的每一个实例中进行处理</span></span><br><span class="line">upperStream.rebalance().print(<span class="string">&quot;RebalancePartitioner&quot;</span>).setParallelism(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这种分区器会根据上下游算子的并行度，循环的方式输出到下游算子的每个实例。</span></span><br><span class="line">upperStream.rescale().print(<span class="string">&quot;RescalePartitioner&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 随机选择一个下游算子实例进行发送</span></span><br><span class="line">upperStream.shuffle().print(<span class="string">&quot;ShufflePartitioner&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 广播分区会将上游数据输出到下游算子的每个实例中。适合于大数据集和小数据集做 join 的场景</span></span><br><span class="line">upperStream.broadcast().print(<span class="string">&quot;BroadcastPartitioner&quot;</span>).setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 发送到下游对应的第一个task。它要求上下游算子并行度一样。如果是操作链 默认为 forward() 以下结果一样</span></span><br><span class="line">upperStream.print(<span class="string">&quot;ForwardPartitioner&quot;</span>).setParallelism(<span class="number">2</span>);</span><br><span class="line">upperStream.forward().print(<span class="string">&quot;ForwardPartitioner&quot;</span>).setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// hashcode % 4 keyBy 默认采用 hashcode 分区器。会将数据按 Key 的 Hash 值输出到下游算子实例中。</span></span><br><span class="line">upperStream.keyBy(word -&gt; word).print(<span class="string">&quot;KeyGroupStreamPartitioner&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用户自定义分区器。需要用户自己实现Partitioner接口，来定义自己的分区逻辑</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink窗口机制</title>
      <link href="/post/6fca1b0d.html"/>
      <url>/post/6fca1b0d.html</url>
      
        <content type="html"><![CDATA[<h3 id="Window-Functions"><a href="#Window-Functions" class="headerlink" title="Window Functions"></a>Window Functions</h3><p>​定义了窗口分配器，我们知道了数据属于哪个窗口，可以将数据收集起来了；至于收集起来到底要做什么，其实还完全没有头绪在窗口分配器之后，必须再接上一个定义窗口如何进行计算的操作，这就是所谓的“窗口函数”（window functions）</p><h4 id="增量聚合函数"><a href="#增量聚合函数" class="headerlink" title="增量聚合函数"></a>增量聚合函数</h4><p>​来一条计算一条，计算的结果存放到一个对象中。</p><p>​<strong>ReduceFunction：</strong>最基本的聚合方式就是归约（reduce）窗口函数中也提供了 ReduceFunction，只要基于 WindowedStream 调用.reduce()方法，然后传入 ReduceFunction 作为参数，就可以<strong>指定以归约两个元素的方式去对窗口中数据进行聚合</strong>了。ReduceFunction 可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是<strong>聚合状态的类型、输出结果的类型都必须和输入数据类型一样</strong>。</p><p>​<strong>AggregateFunction：</strong>AggregateFunction 可以看作是 ReduceFunction 的通用版本，new AggregateFunction&lt;in, acc, out&gt;</p><p>​这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。</p><ul><li>输入类型 IN 就是输入流中元素的数据类型；</li><li>累加器类型 ACC 则是我们进行聚合的中间状态类型；</li><li>而输出类型当然就是最终计算结果的类型了。</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">source.map(word -&gt; Tuple2.of(word.split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>], Integer.parseInt(word.split(<span class="string">&quot;,&quot;</span>)[<span class="number">1</span>])),</span><br><span class="line">        Types.TUPLE(Types.STRING, Types.INT))</span><br><span class="line">    .keyBy(tuple2 -&gt; tuple2.f0)</span><br><span class="line">    .countWindow(<span class="number">3</span>)</span><br><span class="line">    .aggregate(<span class="keyword">new</span> <span class="title class_">AggregateFunction</span>&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;Integer, Integer&gt;, Double&gt;() &#123;</span><br><span class="line">    <span class="comment">// 创建一个累加器，每个聚合任务只会调用一次</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">createAccumulator</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> Tuple2.of(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将输入的元素添加到累加器中，返回一个新的累加器值 acc(sum, count)</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">add</span><span class="params">(Tuple2&lt;String, Integer&gt; input, Tuple2&lt;Integer, Integer&gt; acc)</span> &#123;</span><br><span class="line">        acc.f0 = acc.f0 + input.f1;</span><br><span class="line">        acc.f1 ++;</span><br><span class="line">        <span class="keyword">return</span> acc;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从累加器中提取聚合的输出结果 基于聚合的结果计算出一个最终的结果进行输出</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Double <span class="title function_">getResult</span><span class="params">(Tuple2&lt;Integer, Integer&gt; acc)</span> &#123;</span><br><span class="line">        <span class="comment">// 判断除数不能为0</span></span><br><span class="line">        <span class="keyword">if</span> (acc.f1 == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">return</span> acc.f0 * <span class="number">1.0</span> / acc.f1;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 合并两个累加器，将合并后的状态作为一个累加器返回。这个方法只在需要合并窗口的场景下才会被调用</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">merge</span><span class="params">(Tuple2&lt;Integer, Integer&gt; window1, Tuple2&lt;Integer, Integer&gt; window2)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print(<span class="string">&quot;CountWindow Tumbling &quot;</span>).setParallelism(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h4 id="全量窗口函数"><a href="#全量窗口函数" class="headerlink" title="全量窗口函数"></a>全量窗口函数</h4><p>​ProcessWindowFunction 是 Window API 中最底层的通用窗口函数接口。之所以说它“最底层”，是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction 还可以获取到一个“上下文对象”（Context）。<strong>上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就包括了处理时间（processing time）和事件时间水位线（eventtime watermark）</strong>。</p><p>​全量窗口的好处是以牺牲性能和资源为代价的。作为一个全窗口函数，ProcessWindowFunction 同样需要将所有数据缓存下来、等到窗口触发计算时才使用。它其实就是一个增强版的WindowFunction。</p><h3 id="WaterMark"><a href="#WaterMark" class="headerlink" title="WaterMark"></a>WaterMark</h3><h4 id="内置水位线生成器"><a href="#内置水位线生成器" class="headerlink" title="内置水位线生成器"></a>内置水位线生成器</h4><p>​WatermarkStrategy</p><ul><li>单调递增策略（forMonotonousTimestamps）</li><li>固定乱序长度策略（forBoundedOutOfOrderness）</li><li>不生成策略（noWatermarks）</li></ul><h4 id="自定义水位线"><a href="#自定义水位线" class="headerlink" title="自定义水位线"></a>自定义水位线</h4><p>​<strong>implements WatermarkStrategy&lt;~&gt;</strong></p><p>​Flink有两种不同的生成水位线的方式：一种是周期性的（Periodic），另一种是定点式的（Punctuated）。</p><p>​周期性的（Periodic）：周期性调用的方法中发出水位线 ，Periodic Generator周期性生成器一般是通过 onEvent() 观察判断输入的事件，而在 onPeriodicEmit()里发出水位线</p><p>​定点式的（Punctuated）：在事件触发的方法中发出水位线</p><p>​Punctuated Generator 定点式生成器会不停地检测 onEvent() 中的事件，当发现带有水位线信息的特殊事件时，就立即发出水位线。</p><p>​WatermarkGenerator 接口中有两个方法：</p><ul><li>onEvent()：在每个事件到来时调用</li><li>onPeriodicEmit()：由框架周期性调用</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink任务调度</title>
      <link href="/post/beed7eae.html"/>
      <url>/post/beed7eae.html</url>
      
        <content type="html"><![CDATA[<h3 id="Flink任务调度"><a href="#Flink任务调度" class="headerlink" title="Flink任务调度"></a>Flink任务调度</h3><p>​<strong>JobManagers</strong> ：JobManager有时也叫Masters，主要是协调分布式运行。他们调度任务，协调checkpoint，协调失败任务的恢复等等一个Flink集群中至少有一台JobManager节点。高可用性的集群中将会有多台JobManager节点，其中有一台是leader节点，其他的是备节点(standby)。</p><p>​<strong>TaskManagers</strong> ：TaskManagers有时也叫Workers，TaskManager主要是执行data flow中的任务(tasks)，缓存数据以及进行数据流的交换。TaskManager在同一个JVM中以多线程的方式执行任务TaskManager提供了一定数量的处理插槽（processing slots），用于控制可以并行执行的任务数。每一个集群中至少有一个TaskManager。一个TaskManager可以同时执行多个任务（tasks）</p><ul><li>这些任务可以是同一个算子的子任务（数据并行）</li><li>这些任务可以是来自不同算子（任务并行）</li><li>这些任务可以是另一个不同应用程序（作业并行）</li></ul><p>Flink 是一个分布式流处理框架，任务调度是其核心功能之一。Flink 采用基于事件驱动的调度方式，它可以根据需要在集群中部署和调度一系列可以并发执行的任务，提供了以下几种类型的任务：</p><ol><li>Job：Flink 执行的最基本单元，可用于执行离线批处理、流处理和机器学习等任务。</li><li>Task：一个 Job 由一个或多个 Task 组成，Task 依赖于操作符（Operator），每个操作符负责处理数据流中的一个子任务。</li><li>Subtask：Task 在运行时会被分割成若干个 Subtask 执行，Subtask 维护自己所处理的数据分区，是 Flink 数据并行执行的最小单元。</li></ol><p>任务调度主要包含以下两个方面：</p><ol><li>作业管理：Flink 支持以 JobManager 的形式对作业进行管理，JobManager 负责接收和解析作业提交请求，并将作业拆分成多个 Task，最后将 Task 分配给 TaskManager 执行。</li><li>资源管理：Flink 支持以 ResourceManager 的形式对集群资源进行管理，ResourceManager 负责分配和管理 TaskManager 所需的计算和内存资源。</li></ol><p>具体来说，Flink 的任务调度流程大概如下：</p><ol><li>用户通过客户端向 JobManager 提交作业。</li><li>JobManager 对作业进行解析和划分，并将作业提交给 ResourceManager。</li><li>ResourceManager 根据集群资源的使用情况，为 TaskManager 分配所需的计算和内存资源。</li><li>TaskManager 根据分配的资源启动 Subtask 执行，Subtask 会依次处理数据分区并输出结果。</li><li>JobManager 监控执行状态，如果作业失败或异常终止，则会根据用户定义的策略进行自动重启或手动恢复。</li></ol><p>总的来说，Flink 的任务调度机制是非常健壮和强大的，可以快速、高效地处理各种类型的数据流和处理任务。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink编程模型</title>
      <link href="/post/70157d97.html"/>
      <url>/post/70157d97.html</url>
      
        <content type="html"><![CDATA[<h2 id="Flink编程模型"><a href="#Flink编程模型" class="headerlink" title="Flink编程模型"></a>Flink编程模型</h2><h3 id="分层API"><a href="#分层API" class="headerlink" title="分层API"></a>分层API</h3><p>1、ProcessFunction</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyProcessFunction</span> <span class="keyword">extends</span> <span class="title class_">ProcessFunction</span>&lt;String, String&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> ValueState&lt;Integer&gt; count = <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        ValueStateDescriptor&lt;Integer&gt; descriptor = <span class="keyword">new</span> <span class="title class_">ValueStateDescriptor</span>&lt;&gt;(<span class="string">&quot;count&quot;</span>, Integer.class);</span><br><span class="line">        count = getRuntimeContext().getState(descriptor);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(String value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Integer</span> <span class="variable">currentCount</span> <span class="operator">=</span> count.value();</span><br><span class="line">        <span class="keyword">if</span> (currentCount == <span class="literal">null</span>) &#123;</span><br><span class="line">            currentCount = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        currentCount += <span class="number">1</span>;</span><br><span class="line">        count.update(currentCount);</span><br><span class="line">        out.collect(<span class="string">&quot;Input: &quot;</span> + value + <span class="string">&quot;, Count: &quot;</span> + currentCount);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2、DataStream API</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;String&gt; input = env.addSource(<span class="keyword">new</span> <span class="title class_">FlinkKafkaConsumer</span>&lt;&gt;(<span class="string">&quot;input-topic&quot;</span>, <span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>(), properties));</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; result = input.flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">for</span> (String word : value.split(<span class="string">&quot;\\s&quot;</span>)) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>);</span><br><span class="line">result.print();</span><br></pre></td></tr></table></figure><p>3、SQL &amp; Table API</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">TableEnvironment</span> <span class="variable">tableEnv</span> <span class="operator">=</span> TableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// register a table named &quot;input&quot;</span></span><br><span class="line">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; input = env.fromElements(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>));</span><br><span class="line">tableEnv.registerDataStream(<span class="string">&quot;input&quot;</span>, input, $(<span class="string">&quot;word&quot;</span>), $(<span class="string">&quot;frequency&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// execute a SQL query</span></span><br><span class="line"><span class="type">Table</span> <span class="variable">result</span> <span class="operator">=</span> tableEnv.sqlQuery(<span class="string">&quot;SELECT word, SUM(frequency) as frequency FROM input GROUP BY word&quot;</span>);</span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; resultStream = tableEnv.toRetractStream(result, Row.class);</span><br><span class="line">resultStream.print();</span><br></pre></td></tr></table></figure><h3 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Source</span></span><br><span class="line">Datastream&lt;String&gt; lines = env.addsource <span class="keyword">new</span> <span class="title class_">FlinkKafkaConsumer</span>&lt;&gt;(...))</span><br><span class="line">Datastream&lt;Event&gt; events = lines.map((line) -&gt; parse(line));</span><br><span class="line"><span class="comment">// Transformation</span></span><br><span class="line">Datastream&lt;Statistics&gt; stats = events</span><br><span class="line">    .keyBy(event -&gt; event.id)</span><br><span class="line">    .timeWindow(Time.seconds(<span class="number">10</span>))</span><br><span class="line">    .apply(<span class="keyword">new</span> <span class="title class_">MywindowAggregationFunction</span>());</span><br><span class="line"><span class="comment">// Sink</span></span><br><span class="line">stats.addsink(<span class="keyword">new</span> <span class="title class_">MySink</span>(...));</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
