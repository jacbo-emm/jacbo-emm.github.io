<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Flink算子</title>
      <link href="/post/82ff8bac.html"/>
      <url>/post/82ff8bac.html</url>
      
        <content type="html"><![CDATA[<h3 id="Flink-Source类算子"><a href="#Flink-Source类算子" class="headerlink" title="Flink Source类算子"></a>Flink Source类算子</h3><h4 id="基于文件"><a href="#基于文件" class="headerlink" title="基于文件"></a>基于文件</h4><p>​readTextFile(path) ：读取文本文件，例如遵守 TextInputFormat 规范的文件，逐行读取并将它们作为字符串返回。</p><p>​readFile(fileInputFormat, path) ： 按照指定的文件输入格式读取（一次）文件。</p><h4 id="基于Socket"><a href="#基于Socket" class="headerlink" title="基于Socket"></a>基于Socket</h4><p>​socketTextStream ：从套接字读取。元素可以由分隔符分隔。在启动 Flink 程序之前， 必须先启动一个 Socket 服务</p><h4 id="基于集合"><a href="#基于集合" class="headerlink" title="基于集合"></a>基于集合</h4><p>​fromCollection(Collection) ：从 Java Java.util.Collection 创建数据流。集合中的所有元素必须属于同一类型。</p><p>​fromCollection(Iterator, Class) ：从迭代器创建数据流。class 参数指定迭代器返回元素的数据类型。</p><p>​fromElements(T …) ： 从给定的对象序列中创建数据流。所有的对象必须属于同一类型。</p><p>​fromParallelCollection(SplittableIterator, Class) ：从迭代器并行创建数据流。class 参数指定迭代器返回元素的数据类型。</p><p>​generateSequence(from, to) ：基于给定间隔内的数字序列并行生成数据流。</p><h4 id="基于Connectors"><a href="#基于Connectors" class="headerlink" title="基于Connectors"></a>基于Connectors</h4><p>​预定义 data sources 支持从文件、目录、socket，以及 collections 和 iterators 中读取数据。</p><h3 id="Flink-Transformation类算子"><a href="#Flink-Transformation类算子" class="headerlink" title="Flink Transformation类算子"></a>Flink Transformation类算子</h3><h4 id="map和flatMap"><a href="#map和flatMap" class="headerlink" title="map和flatMap"></a>map和flatMap</h4><p>​flatMap 算子会将每条输入记录映射为多条输出记录，并将这些输出记录展开成一个扁平的数据集。与之相对的算子是 map 算子，它只将每条输入记录映射为一条输出记录。</p><h4 id="keyBy"><a href="#keyBy" class="headerlink" title="keyBy"></a>keyBy</h4><p>​在逻辑上将流划分为不相交的分区。具有相同 key 的记录都分配到同一个分区。在内部， <em>keyBy()</em> 是通过哈希分区实现的。有多种指定 key 的方式。</p><p>​以下情况，一个类<strong>不能作为 key</strong>：</p><ol><li>它是一种 POJO 类，但没有重写 hashCode() 方法而是依赖于 Object.hashCode() 实现。</li><li>它是任意类的数组。</li></ol><h4 id="aggregation"><a href="#aggregation" class="headerlink" title="aggregation"></a>aggregation</h4><p>​滚动聚合算子由 KeyedStream 调用，并生成一个聚合以后的DataStream滚动聚合算子是多个聚合算子的统称， 有 sum、 min、 minBy、 max、 maxBy；</p><p>​滚动聚合方法：</p><p>​sum()：在输入流上对指定的字段做滚动相加操作。</p><p>​min()：在输入流上对指定的字段求最小值。</p><p>​max()：在输入流上对指定的字段求最大值。</p><p>​minBy()：在输入流上针对指定字段求最小值，并返回最小值字段所在的那条数据。</p><p>​maxBy()：在输入流上针对指定字段求最大值，并返回最大值字段所在的那条数据。</p><h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h4><p>​在相同 key 的数据流上“滚动”执行 reduce。将当前元素与最后一次 reduce 得到的值组合然后输出新值。</p><h4 id="iterate"><a href="#iterate" class="headerlink" title="iterate"></a>iterate</h4><p>​通过将一个算子的输出重定向到某个之前的算子来在流中创建“反馈”循环。这对于定义持续更新模型的算法特别有用。下面的代码从一个流开始，并不断地应用迭代自身。大于 0 的元素被发送回反馈通道，其余元素被转发到下游。</p><h4 id="union和connect"><a href="#union和connect" class="headerlink" title="union和connect"></a>union和connect</h4><p>​在<code>DataStream</code>上使用<code>union</code>算子可以合并多个同类型的数据流，并生成同类型的数据流，即可以将多个<code>DataStream[T]</code>合并为一个新的<code>DataStream[T]</code>。数据将按照先进先出（First In First Out）的模式合并，且不去重。下图<code>union</code>对白色和深色两个数据流进行合并，生成一个数据流。</p><p>​<code>union</code>虽然可以合并多个数据流，但有一个限制，即多个数据流的数据类型必须相同。<code>connect</code>提供了和<code>union</code>类似的功能，用来连接两个数据流，它与<code>union</code>的区别在于：</p><ol><li><code>connect</code>只能连接两个数据流，<code>union</code>可以连接多个数据流。</li><li><code>connect</code>所连接的两个数据流的数据类型可以不一致，<code>union</code>所连接的两个数据流的数据类型必须一致。</li><li>两个<code>DataStream</code>经过<code>connect</code>之后被转化为<code>ConnectedStreams</code>，<code>ConnectedStreams</code>会对两个流的数据应用不同的处理方法，且双流之间可以共享状态。</li></ol><p><code>connect</code>经常被应用在对一个数据流使用另外一个流进行控制处理的场景上，如下图所示。控制流可以是阈值、规则、机器学习模型或其他参数。</p><h3 id="Flink-Sink类算子"><a href="#Flink-Sink类算子" class="headerlink" title="Flink Sink类算子"></a>Flink Sink类算子</h3><p>​writeToSocket：将计算结果输出到某台机器的端口上。</p><p>​预定义 data sinks 支持把数据写入文件、标准输出（stdout）、标准错误输出（stderr）和 socket</p><h3 id="Flink-Partitioner类算子"><a href="#Flink-Partitioner类算子" class="headerlink" title="Flink Partitioner类算子"></a>Flink Partitioner类算子</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">upperStream.print().setParallelism(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// GlobalPartitioner 分区器会将上游所有元素都发送到下游的第一个算子实例上(SubTask id = 0)</span></span><br><span class="line">upperStream.global().print(<span class="string">&quot;GlobalPartitioner&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据会被循环发送到下游的每一个实例中进行处理</span></span><br><span class="line">upperStream.rebalance().print(<span class="string">&quot;RebalancePartitioner&quot;</span>).setParallelism(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这种分区器会根据上下游算子的并行度，循环的方式输出到下游算子的每个实例。</span></span><br><span class="line">upperStream.rescale().print(<span class="string">&quot;RescalePartitioner&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 随机选择一个下游算子实例进行发送</span></span><br><span class="line">upperStream.shuffle().print(<span class="string">&quot;ShufflePartitioner&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 广播分区会将上游数据输出到下游算子的每个实例中。适合于大数据集和小数据集做 join 的场景</span></span><br><span class="line">upperStream.broadcast().print(<span class="string">&quot;BroadcastPartitioner&quot;</span>).setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 发送到下游对应的第一个task。它要求上下游算子并行度一样。如果是操作链 默认为 forward() 以下结果一样</span></span><br><span class="line">upperStream.print(<span class="string">&quot;ForwardPartitioner&quot;</span>).setParallelism(<span class="number">2</span>);</span><br><span class="line">upperStream.forward().print(<span class="string">&quot;ForwardPartitioner&quot;</span>).setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// hashcode % 4 keyBy 默认采用 hashcode 分区器。会将数据按 Key 的 Hash 值输出到下游算子实例中。</span></span><br><span class="line">upperStream.keyBy(word -&gt; word).print(<span class="string">&quot;KeyGroupStreamPartitioner&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用户自定义分区器。需要用户自己实现Partitioner接口，来定义自己的分区逻辑</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink窗口机制</title>
      <link href="/post/6fca1b0d.html"/>
      <url>/post/6fca1b0d.html</url>
      
        <content type="html"><![CDATA[<h3 id="Window-Functions"><a href="#Window-Functions" class="headerlink" title="Window Functions"></a>Window Functions</h3><p>​定义了窗口分配器，我们知道了数据属于哪个窗口，可以将数据收集起来了；至于收集起来到底要做什么，其实还完全没有头绪在窗口分配器之后，必须再接上一个定义窗口如何进行计算的操作，这就是所谓的“窗口函数”（window functions）</p><h4 id="增量聚合函数"><a href="#增量聚合函数" class="headerlink" title="增量聚合函数"></a>增量聚合函数</h4><p>​来一条计算一条，计算的结果存放到一个对象中。</p><p>​<strong>ReduceFunction：</strong>最基本的聚合方式就是归约（reduce）窗口函数中也提供了 ReduceFunction，只要基于 WindowedStream 调用.reduce()方法，然后传入 ReduceFunction 作为参数，就可以<strong>指定以归约两个元素的方式去对窗口中数据进行聚合</strong>了。ReduceFunction 可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是<strong>聚合状态的类型、输出结果的类型都必须和输入数据类型一样</strong>。</p><p>​<strong>AggregateFunction：</strong>AggregateFunction 可以看作是 ReduceFunction 的通用版本，new AggregateFunction&lt;in, acc, out&gt;</p><p>​这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。</p><ul><li>输入类型 IN 就是输入流中元素的数据类型；</li><li>累加器类型 ACC 则是我们进行聚合的中间状态类型；</li><li>而输出类型当然就是最终计算结果的类型了。</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">source.map(word -&gt; Tuple2.of(word.split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>], Integer.parseInt(word.split(<span class="string">&quot;,&quot;</span>)[<span class="number">1</span>])),</span><br><span class="line">        Types.TUPLE(Types.STRING, Types.INT))</span><br><span class="line">    .keyBy(tuple2 -&gt; tuple2.f0)</span><br><span class="line">    .countWindow(<span class="number">3</span>)</span><br><span class="line">    .aggregate(<span class="keyword">new</span> <span class="title class_">AggregateFunction</span>&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;Integer, Integer&gt;, Double&gt;() &#123;</span><br><span class="line">    <span class="comment">// 创建一个累加器，每个聚合任务只会调用一次</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">createAccumulator</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> Tuple2.of(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将输入的元素添加到累加器中，返回一个新的累加器值 acc(sum, count)</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">add</span><span class="params">(Tuple2&lt;String, Integer&gt; input, Tuple2&lt;Integer, Integer&gt; acc)</span> &#123;</span><br><span class="line">        acc.f0 = acc.f0 + input.f1;</span><br><span class="line">        acc.f1 ++;</span><br><span class="line">        <span class="keyword">return</span> acc;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从累加器中提取聚合的输出结果 基于聚合的结果计算出一个最终的结果进行输出</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Double <span class="title function_">getResult</span><span class="params">(Tuple2&lt;Integer, Integer&gt; acc)</span> &#123;</span><br><span class="line">        <span class="comment">// 判断除数不能为0</span></span><br><span class="line">        <span class="keyword">if</span> (acc.f1 == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">return</span> acc.f0 * <span class="number">1.0</span> / acc.f1;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 合并两个累加器，将合并后的状态作为一个累加器返回。这个方法只在需要合并窗口的场景下才会被调用</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">merge</span><span class="params">(Tuple2&lt;Integer, Integer&gt; window1, Tuple2&lt;Integer, Integer&gt; window2)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print(<span class="string">&quot;CountWindow Tumbling &quot;</span>).setParallelism(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h4 id="全量窗口函数"><a href="#全量窗口函数" class="headerlink" title="全量窗口函数"></a>全量窗口函数</h4><p>​ProcessWindowFunction 是 Window API 中最底层的通用窗口函数接口。之所以说它“最底层”，是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction 还可以获取到一个“上下文对象”（Context）。<strong>上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就包括了处理时间（processing time）和事件时间水位线（eventtime watermark）</strong>。</p><p>​全量窗口的好处是以牺牲性能和资源为代价的。作为一个全窗口函数，ProcessWindowFunction 同样需要将所有数据缓存下来、等到窗口触发计算时才使用。它其实就是一个增强版的WindowFunction。</p><h3 id="WaterMark"><a href="#WaterMark" class="headerlink" title="WaterMark"></a>WaterMark</h3><h4 id="内置水位线生成器"><a href="#内置水位线生成器" class="headerlink" title="内置水位线生成器"></a>内置水位线生成器</h4><p>​WatermarkStrategy</p><ul><li>单调递增策略（forMonotonousTimestamps）</li><li>固定乱序长度策略（forBoundedOutOfOrderness）</li><li>不生成策略（noWatermarks）</li></ul><h4 id="自定义水位线"><a href="#自定义水位线" class="headerlink" title="自定义水位线"></a>自定义水位线</h4><p>​<strong>implements WatermarkStrategy&lt;~&gt;</strong></p><p>​Flink有两种不同的生成水位线的方式：一种是周期性的（Periodic），另一种是定点式的（Punctuated）。</p><p>​周期性的（Periodic）：周期性调用的方法中发出水位线 ，Periodic Generator周期性生成器一般是通过 onEvent() 观察判断输入的事件，而在 onPeriodicEmit()里发出水位线</p><p>​定点式的（Punctuated）：在事件触发的方法中发出水位线</p><p>​Punctuated Generator 定点式生成器会不停地检测 onEvent() 中的事件，当发现带有水位线信息的特殊事件时，就立即发出水位线。</p><p>​WatermarkGenerator 接口中有两个方法：</p><ul><li>onEvent()：在每个事件到来时调用</li><li>onPeriodicEmit()：由框架周期性调用</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink任务调度</title>
      <link href="/post/beed7eae.html"/>
      <url>/post/beed7eae.html</url>
      
        <content type="html"><![CDATA[<h3 id="Flink任务调度"><a href="#Flink任务调度" class="headerlink" title="Flink任务调度"></a>Flink任务调度</h3><p>​<strong>JobManagers</strong> ：JobManager有时也叫Masters，主要是协调分布式运行。他们调度任务，协调checkpoint，协调失败任务的恢复等等一个Flink集群中至少有一台JobManager节点。高可用性的集群中将会有多台JobManager节点，其中有一台是leader节点，其他的是备节点(standby)。</p><p>​<strong>TaskManagers</strong> ：TaskManagers有时也叫Workers，TaskManager主要是执行data flow中的任务(tasks)，缓存数据以及进行数据流的交换。TaskManager在同一个JVM中以多线程的方式执行任务TaskManager提供了一定数量的处理插槽（processing slots），用于控制可以并行执行的任务数。每一个集群中至少有一个TaskManager。一个TaskManager可以同时执行多个任务（tasks）</p><ul><li>这些任务可以是同一个算子的子任务（数据并行）</li><li>这些任务可以是来自不同算子（任务并行）</li><li>这些任务可以是另一个不同应用程序（作业并行）</li></ul><p>Flink 是一个分布式流处理框架，任务调度是其核心功能之一。Flink 采用基于事件驱动的调度方式，它可以根据需要在集群中部署和调度一系列可以并发执行的任务，提供了以下几种类型的任务：</p><ol><li>Job：Flink 执行的最基本单元，可用于执行离线批处理、流处理和机器学习等任务。</li><li>Task：一个 Job 由一个或多个 Task 组成，Task 依赖于操作符（Operator），每个操作符负责处理数据流中的一个子任务。</li><li>Subtask：Task 在运行时会被分割成若干个 Subtask 执行，Subtask 维护自己所处理的数据分区，是 Flink 数据并行执行的最小单元。</li></ol><p>任务调度主要包含以下两个方面：</p><ol><li>作业管理：Flink 支持以 JobManager 的形式对作业进行管理，JobManager 负责接收和解析作业提交请求，并将作业拆分成多个 Task，最后将 Task 分配给 TaskManager 执行。</li><li>资源管理：Flink 支持以 ResourceManager 的形式对集群资源进行管理，ResourceManager 负责分配和管理 TaskManager 所需的计算和内存资源。</li></ol><p>具体来说，Flink 的任务调度流程大概如下：</p><ol><li>用户通过客户端向 JobManager 提交作业。</li><li>JobManager 对作业进行解析和划分，并将作业提交给 ResourceManager。</li><li>ResourceManager 根据集群资源的使用情况，为 TaskManager 分配所需的计算和内存资源。</li><li>TaskManager 根据分配的资源启动 Subtask 执行，Subtask 会依次处理数据分区并输出结果。</li><li>JobManager 监控执行状态，如果作业失败或异常终止，则会根据用户定义的策略进行自动重启或手动恢复。</li></ol><p>总的来说，Flink 的任务调度机制是非常健壮和强大的，可以快速、高效地处理各种类型的数据流和处理任务。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink编程模型</title>
      <link href="/post/70157d97.html"/>
      <url>/post/70157d97.html</url>
      
        <content type="html"><![CDATA[<h2 id="Flink编程模型"><a href="#Flink编程模型" class="headerlink" title="Flink编程模型"></a>Flink编程模型</h2><h3 id="分层API"><a href="#分层API" class="headerlink" title="分层API"></a>分层API</h3><p>1、ProcessFunction</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyProcessFunction</span> <span class="keyword">extends</span> <span class="title class_">ProcessFunction</span>&lt;String, String&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> ValueState&lt;Integer&gt; count = <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        ValueStateDescriptor&lt;Integer&gt; descriptor = <span class="keyword">new</span> <span class="title class_">ValueStateDescriptor</span>&lt;&gt;(<span class="string">&quot;count&quot;</span>, Integer.class);</span><br><span class="line">        count = getRuntimeContext().getState(descriptor);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(String value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Integer</span> <span class="variable">currentCount</span> <span class="operator">=</span> count.value();</span><br><span class="line">        <span class="keyword">if</span> (currentCount == <span class="literal">null</span>) &#123;</span><br><span class="line">            currentCount = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        currentCount += <span class="number">1</span>;</span><br><span class="line">        count.update(currentCount);</span><br><span class="line">        out.collect(<span class="string">&quot;Input: &quot;</span> + value + <span class="string">&quot;, Count: &quot;</span> + currentCount);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2、DataStream API</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;String&gt; input = env.addSource(<span class="keyword">new</span> <span class="title class_">FlinkKafkaConsumer</span>&lt;&gt;(<span class="string">&quot;input-topic&quot;</span>, <span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>(), properties));</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; result = input.flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">for</span> (String word : value.split(<span class="string">&quot;\\s&quot;</span>)) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>);</span><br><span class="line">result.print();</span><br></pre></td></tr></table></figure><p>3、SQL &amp; Table API</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">TableEnvironment</span> <span class="variable">tableEnv</span> <span class="operator">=</span> TableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// register a table named &quot;input&quot;</span></span><br><span class="line">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; input = env.fromElements(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>));</span><br><span class="line">tableEnv.registerDataStream(<span class="string">&quot;input&quot;</span>, input, $(<span class="string">&quot;word&quot;</span>), $(<span class="string">&quot;frequency&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// execute a SQL query</span></span><br><span class="line"><span class="type">Table</span> <span class="variable">result</span> <span class="operator">=</span> tableEnv.sqlQuery(<span class="string">&quot;SELECT word, SUM(frequency) as frequency FROM input GROUP BY word&quot;</span>);</span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; resultStream = tableEnv.toRetractStream(result, Row.class);</span><br><span class="line">resultStream.print();</span><br></pre></td></tr></table></figure><h3 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Source</span></span><br><span class="line">Datastream&lt;String&gt; lines = env.addsource <span class="keyword">new</span> <span class="title class_">FlinkKafkaConsumer</span>&lt;&gt;(...))</span><br><span class="line">Datastream&lt;Event&gt; events = lines.map((line) -&gt; parse(line));</span><br><span class="line"><span class="comment">// Transformation</span></span><br><span class="line">Datastream&lt;Statistics&gt; stats = events</span><br><span class="line">    .keyBy(event -&gt; event.id)</span><br><span class="line">    .timeWindow(Time.seconds(<span class="number">10</span>))</span><br><span class="line">    .apply(<span class="keyword">new</span> <span class="title class_">MywindowAggregationFunction</span>());</span><br><span class="line"><span class="comment">// Sink</span></span><br><span class="line">stats.addsink(<span class="keyword">new</span> <span class="title class_">MySink</span>(...));</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Flink学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
